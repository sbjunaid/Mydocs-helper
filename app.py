import os
import streamlit as st
from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama  # ‚úÖ Ollama auto-runs locally

# ‚úÖ Title
st.title("üß† GitHub Knowledge Assistant (Offline with Gemma3 via Ollama)")

# ‚úÖ Load .txt files
def load_docs():
    docs = []
    for filename in os.listdir("docs"):
        if filename.endswith(".txt"):
            path = os.path.join("docs", filename)
            try:
                with open(path, "r", encoding="utf-8") as file:
                    content = file.read()
                    docs.append(Document(page_content=content, metadata={"source": filename}))
                    st.write(f"‚úÖ Loaded: {filename}")
            except Exception as e:
                st.error(f"‚ùå Error reading {filename}: {e}")
    return docs

# ‚úÖ Get user query
query = st.text_input("Ask something about GitHub, Python, debugging, etc:")

if query:
    with st.spinner("üîÑ Processing..."):
        try:
            # Load documents
            documents = load_docs()
            st.write(f"üìÑ Total documents loaded: {len(documents)}")
            if not documents:
                st.warning("‚ö†Ô∏è No documents found in 'docs' folder.")
                st.stop()

            # Split into chunks
            st.write("üìé Splitting documents into chunks...")
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=800, chunk_overlap=100, separators=["\n\n", "\n", ".", " "]
            )
            texts = splitter.split_documents(documents)
            st.write(f"üìé Total chunks created: {len(texts)}")

            # Embeddings
            st.write("üß† Creating embeddings...")
            embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
            st.write("‚úÖ Embeddings ready.")

            # Vector store
            st.write("üß† Creating vector store...")
            vectorstore = Chroma.from_documents(texts, embeddings)
            st.write("‚úÖ Vector store created.")

            # üß† Ollama (auto-runs Gemma3 in background)
            st.write("ü§ñ Loading local Gemma3 model via Ollama...")
            llm = Ollama(model="gemma3", temperature=0.3)
            st.write("‚úÖ Model loaded.")

            # Build QA
            st.write("üîó Building QA chain...")
            qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                retriever=vectorstore.as_retriever(search_type="similarity", k=3),
                return_source_documents=True
            )
            st.write("‚úÖ QA chain ready.")

            # Run Query
            with st.spinner("ü§ñ Thinking..."):
                st.write("üí¨ Running query...")
                response = qa_chain(query)
                st.success(response["result"])

                with st.expander("üîç Sources used"):
                    for doc in response["source_documents"]:
                        st.markdown(f"üìÑ **{doc.metadata['source']}**")
                        st.code(doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content)

        except Exception as e:
            st.error(f"‚ùå Error in pipeline: {e}")
