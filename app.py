import os
import streamlit as st
from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama  # âœ… Ollama auto-runs locally

# âœ… Title
st.title("ğŸ§  Mydoc Assistant (Offline with Gemma3 via Ollama)")

# âœ… Load .txt files
def load_docs():
    docs = []
    for filename in os.listdir("docs"):
        if filename.endswith(".txt"):
            path = os.path.join("docs", filename)
            try:
                with open(path, "r", encoding="utf-8") as file:
                    content = file.read()
                    docs.append(Document(page_content=content, metadata={"source": filename}))
                    st.write(f"âœ… Loaded: {filename}")
            except Exception as e:
                st.error(f"âŒ Error reading {filename}: {e}")
    return docs

# âœ… Get user query
query = st.text_input("Ask something about GitHub, Python, debugging, etc:")

if query:
    with st.spinner("ğŸ”„ Processing..."):
        try:
            # Load documents
            documents = load_docs()
            st.write(f"ğŸ“„ Total documents loaded: {len(documents)}")
            if not documents:
                st.warning("âš ï¸ No documents found in 'docs' folder.")
                st.stop()

            # Split into chunks
            st.write("ğŸ“ Splitting documents into chunks...")
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=800, chunk_overlap=100, separators=["\n\n", "\n", ".", " "]
            )
            texts = splitter.split_documents(documents)
            st.write(f"ğŸ“ Total chunks created: {len(texts)}")

            # Embeddings
            st.write("ğŸ§  Creating embeddings...")
            embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
            st.write("âœ… Embeddings ready.")

            # Vector store
            st.write("ğŸ§  Creating vector store...")
            vectorstore = Chroma.from_documents(texts, embeddings)
            st.write("âœ… Vector store created.")

            # ğŸ§  Ollama (auto-runs Gemma3 in background)
            st.write("ğŸ¤– Loading local Gemma3 model via Ollama...")
            llm = Ollama(model="gemma3", temperature=0.3)
            st.write("âœ… Model loaded.")

            # Build QA
            st.write("ğŸ”— Building QA chain...")
            qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                retriever=vectorstore.as_retriever(search_type="similarity", k=3),
                return_source_documents=True
            )
            st.write("âœ… QA chain ready.")

            # Run Query
            with st.spinner("ğŸ¤– Thinking..."):
                st.write("ğŸ’¬ Running query...")
                response = qa_chain(query)
                st.success(response["result"])

                with st.expander("ğŸ” Sources used"):
                    for doc in response["source_documents"]:
                        st.markdown(f"ğŸ“„ **{doc.metadata['source']}**")
                        st.code(doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content)

        except Exception as e:
            st.error(f"âŒ Error in pipeline: {e}")
st.markdown(
    """
    <hr style="margin-top: 50px;">
    <div style='text-align: center; font-size: 14px; color: gray;'>
        Made with ğŸ’», ğŸ˜¤, and â˜• by <strong>Sayed Bakhtiar Junaid</strong> ğŸ™‹â€â™‚ï¸.
    </div>
    """,
    unsafe_allow_html=True
)
